{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CSCI 381/780 (Fall 2022) - Project 2\n",
    "\n",
    "**Due Date: Monday, November 14 by 4 PM**\n",
    "\n",
    "## Description\n",
    "In this project you will construct machine learning models on two different real-world datasets using unsupervised learning and regression.\n",
    "\n",
    "## Instructions\n",
    "1. In this project you will write code to construct machine learning models and write responses to questions concerning the performance of said models. Please complete all sections below, adding new *Code* or *Markdown* cells as appropriate to answer the questions.\n",
    "2. There are many Scikit-learn functions that leverage randomness to generate results. For these functions, a pseudorandom generator can be initialized using a seed value by passing the parameter `random_state=XXX`, where `XXX` is some number between 1 and 2^31-1. For each of these functions, **you will utilize your CUNY ID number** to initialize the function. Functions include:\n",
    "- `ShuffleSplit`\n",
    "- `KFold`\n",
    "- `KMeans`\n",
    "- `GridSearchCV`\n",
    "- `Lasso`\n",
    "- `MLPRegressor`\n",
    "3. You will **work independently** on the project. Please make use of the *Python Data Science Reference Materials* posted on Blackboard, or **come to office hours should you need further assistance**.\n",
    "4. You will submit a single Jupyter notebook containing all code and written responses via Blackboard by the due date listed above.\n",
    "\n",
    "## Grading\n",
    "\n",
    "### Running Code\n",
    "Your Jupyter notebook must be able to run from start to finish **without error**. Please turn any cell that contains scratch work or other non-executable items to *Raw*. **Notebooks that cannot run to completion will receive a grade of 0**.\n",
    "\n",
    "\n",
    "### Rubric\n",
    "\n",
    "|**Part**|1.1|1.2|2.1|2.2|2.3|2.4|2.5|2.6|**Total**|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**%**|10|30|15|5|10|10|10|10|100|\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this part of the project, you will be using data collected by the US Geological Survey and the US Forest Service. The data describes various cartographic and geologic features related to forest cover in US wilderness areas, with each sample representing a 30 x 30 meter cell.\n",
    "\n",
    "The **goal** will be to *identify clusters* that represent forest cover types, and using these clusters *extract relationships* between forest covers and the provided cartographic/geologic features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "cover_column_names=[\"Elevation\",\"Aspect\",\"Slope\",\n",
    "              \"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\n",
    "              \"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\n",
    "              \"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"]\n",
    "wilderness_cols =[\"Wilderness_\"+str(i) for i in range(4)]\n",
    "soil_col =[\"Soil_\"+str(i) for i in range(40)]\n",
    "cover_columns_names = cover_column_names + wilderness_cols + soil_col"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Load Data\n",
    "Set the variable `COVER_FILE` to the **full path** to the forest cover dataset (**forest_cover_dataset.csv**) on your system. Load the file into a dataframe (you may initialize the column names using the header list `cover_column_names`), then:\n",
    "1. Determine the number and types of features.\n",
    "2. Perform a **ShuffleSplit** of the data into training/validation/test sets, 60%/20%/20%.\n",
    "3. **Center** the training/validation/test splits (fit on the training set, then transform the validation/test sets)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cover_file = f'C:/Users/tanzi/CS Lang IDE/PycharmProjects/Project-2/UTF-8_forest_cover_dataset.csv'\n",
    "#search later, how to auto complete full path. https://www.youtube.com/watch?v=3Spa10-mwsw\n",
    "\n",
    "cover_file = f'C:/Users/tanzi/CS Lang IDE/PycharmProjects/Jonathan/Project2/Project-2/UTF-8_forest_cover_dataset.csv'\n",
    "forest_data = pd.read_csv(cover_file, names=cover_columns_names)\n",
    "forest_data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "num_features = len(cover_columns_names)\n",
    "print('Number of features: ', num_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#change the variable name\n",
    "\n",
    "#Determine nature of object types\n",
    "crime_cat_columns=dict()\n",
    "crime_cat_columns_idx=dict()\n",
    "crime_num_columns_idx=dict()\n",
    "for col in cover_columns_names:\n",
    "    if forest_data[col].dtype == 'int64':\n",
    "        crime_cat_columns[col]=forest_data[col].unique()\n",
    "        crime_cat_columns_idx[col]=forest_data.columns.get_loc(col)\n",
    "        print(col,\":\", crime_cat_columns[col])\n",
    "    elif np.issubdtype(forest_data.at[0,col], np.number):\n",
    "        crime_num_columns_idx[col]=forest_data.columns.get_loc(col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Perform a **ShuffleSplit** of the data into training/validation/test sets, 60%/20%/20%.\n",
    "forest_data_list = np.array(forest_data)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "split_test = ShuffleSplit(n_splits=1,test_size=0.2, random_state=23508893)\n",
    "for train_index, test_index in split_test.split(forest_data_list):\n",
    "    forest_train_proj_full = forest_data_list[train_index,:]\n",
    "    forest_test_proj = forest_data_list[test_index,:]\n",
    "\n",
    "\n",
    "split_validation = ShuffleSplit(n_splits=1,test_size=0.25, random_state=23508893)\n",
    "for train_index, validation_index in split_validation.split(forest_train_proj_full):\n",
    "    forest_train_proj = forest_train_proj_full[train_index,:]\n",
    "    forest_validation_proj = forest_train_proj_full[validation_index,:]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #Maybe comment this out?\n",
    "# print(\"Dataset\\t\\tPercent\")\n",
    "# print(\"Training\\t\" + str(np.sum(forest_train_proj) / len(forest_train_proj)))\n",
    "# print(\"Validation\\t\" + str(np.sum(forest_validation_proj) / len(forest_validation_proj)))\n",
    "# print(\"Test\\t\\t\" + str(np.sum(forest_test_proj) / len(forest_test_proj)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. How would want us to solve 1.1.3 [center the training]. Should I use customs transformer? If so, during the next step, should I use data_centered_train or forest_train_proj?\n",
    "2. forest_train_proj = dataset (X) | data_center_train = first center randomly\n",
    "3.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Customs transformers\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "center_function = lambda x: x-x.mean()\n",
    "transformer = FunctionTransformer(center_function, validate=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "svm_scaler = StandardScaler(with_std=False)\n",
    "forest_train_proj_scaled=svm_scaler.fit_transform(forest_train_proj)\n",
    "forest_validation_proj_scaled=svm_scaler.transform(forest_validation_proj)\n",
    "forest_test_proj_scaled=svm_scaler.transform(forest_test_proj)\n",
    "forest_train_proj_full_scaled=svm_scaler.transform(forest_train_proj_full)\n",
    "\n",
    "forest_train_proj_full_scaled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 $k$-Means Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Cluster the *training* dataset using ```KMeans``` from Scikit-learn using the values for $k$ below. You can utilize the default implementation in Scikit-learn, which is $k$-means$++$. Construct models for each value of $k$ specified in the ``k_values`` array below.\n",
    "2. Apply the trained models to the validation dataset.\n",
    "3. Compute the **mean** inertia for each value of $k$ on the training and validation datasets. For the training data, you can extract the *total* (**unnormalized**) inertia from the trained models via their respective ```inertia_```. For the validation models, you can utilize the function ```total_inertia``` provided below.\n",
    "4. Plot the mean inertias for all values of $k$ and for both the training and validation sets on a single plot. Based on the elbow method, which value of $k$ should be chosen so that the model will generalize to new data?\n",
    "5. The *Silhouette Coefficient* is another cluster performance metric that combines intra-cluster distance with inter-cluster distances from clusters in close proximity to each other. Utilize Scikit-learn's ```silhouette_score``` function [(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) to compute the silhouette coefficient for all values of $k$ on the *validation* set. Plot these values and determine via the elbow method which value of $k$ should be chosen. How does this value compare to the one chosen using mean inertia?\n",
    "6. Using whichever value of $k$ you think is the best based on steps 4) and 5), create a plot for each feature that contains [boxplots](https://matplotlib.org/stable/gallery/statistics/boxplot_demo.html#sphx-glr-gallery-statistics-boxplot-demo-py) of that feature for each cluster. Based upon these plots, determine which features you think are meaningful in discriminating between the clusters, as well as any other trends you observe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Please Read!\n",
    "You may benefit from parallelizing the creation of the $k$-means models by using the Python package ```joblib```. I've included a code template below that you can customize. For additional details, please refer to [joblib's documentation](https://joblib.readthedocs.io/en/latest/parallel.html)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cluster the *training* dataset using ```KMeans```. Construct models for each value of $k$ specified in the ``k_values`` array\n",
    "### Comment:\n",
    "1. Should I use fit or fit_transform?\n",
    "2. fit_predict() vs predict() <- I think I got my answer| labels vs predict; what is labels?\n",
    "3. whats the difference between writing out the parameter for iniit: 'kmean++' or not?\n",
    "The reason why I ask, because I don't see any data difference with or without, even after adding n_init=k, where is interger of the list from k_value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_values=list(range(2,11))+[15,20]\n",
    "\n",
    "#n-clusters = int [not list]; figure out a way to get int.\n",
    "from sklearn.cluster import KMeans\n",
    "#fit: kmeans return type: kmeans\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k,random_state=23508893).fit(forest_train_proj_scaled) # fit vs fit_transform?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.89987973e+02,  2.42573668e+01, -2.90503297e+00, ...,\n         5.79523816e-02, -4.05053971e-03, -9.05738924e-03],\n       [-4.08931518e+01,  1.53860189e-01,  8.14643524e-01, ...,\n        -9.26610931e-04, -2.78386691e-03, -1.54462483e-03],\n       [ 1.47750060e+01,  1.81121976e+01, -3.58351762e+00, ...,\n        -1.66815343e-02, -1.46000595e-02, -9.05738924e-03],\n       ...,\n       [ 4.50070242e+01, -6.98490411e+00, -3.09770729e-01, ...,\n         1.20062470e-03, -1.02325275e-02,  3.00031794e-02],\n       [ 6.00761284e+00,  2.60392145e+01,  1.06192993e-01, ...,\n        -1.50209220e-02, -1.26280823e-02, -8.12329479e-03],\n       [ 1.08748155e+02, -4.17107911e+01, -3.16187906e+00, ...,\n         9.59288739e-03,  1.86359612e-02,  6.43778256e-03]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apply the trained models to the validation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "array([11, 12, 13, ...,  7, 17,  4])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_predict = kmeans.predict(forest_validation_proj_scaled)\n",
    "valid_predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.89987973e+02,  2.42573668e+01, -2.90503297e+00, ...,\n         5.79523816e-02, -4.05053971e-03, -9.05738924e-03],\n       [-4.08931518e+01,  1.53860189e-01,  8.14643524e-01, ...,\n        -9.26610931e-04, -2.78386691e-03, -1.54462483e-03],\n       [ 1.47750060e+01,  1.81121976e+01, -3.58351762e+00, ...,\n        -1.66815343e-02, -1.46000595e-02, -9.05738924e-03],\n       ...,\n       [ 4.50070242e+01, -6.98490411e+00, -3.09770729e-01, ...,\n         1.20062470e-03, -1.02325275e-02,  3.00031794e-02],\n       [ 6.00761284e+00,  2.60392145e+01,  1.06192993e-01, ...,\n        -1.50209220e-02, -1.26280823e-02, -8.12329479e-03],\n       [ 1.08748155e+02, -4.17107911e+01, -3.16187906e+00, ...,\n         9.59288739e-03,  1.86359612e-02,  6.43778256e-03]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Comment\n",
    "1. Is the func supposed to be for loop of the kvalues? and the data is train_model?\n",
    "2. Create a function for for-loop, where it takes list, data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd723091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed, parallel_backend\n",
    "#\n",
    "# with parallel_backend(\"loky\", inner_max_num_threads=2):\n",
    "#     results = Parallel(n_jobs=4)(delayed(func)(x, y) for x, y in data)\n",
    "#     # which data?\n",
    "#     # || x = forest_train_proj and y = data_centered_train?\n",
    "#     # || x = k-mean and y = k-mean ++?\n",
    "#     # || x = forest_train_proj and y = k-means\n",
    "#     # func = center_function? or [cluster_centers_ or labels_] from k-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38cf5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_inertia(dataset,centers,labels,distance=None,mean=True):\n",
    "    # dataset= forest_validation_proj | centers = kmeans.cluster_centers_ [valid_predict] | label = valid_predict |mean=false\n",
    "    \"\"\"\n",
    "    Computes the total inertia (the intracluster variance) given a dataset and the cluster centers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : numpy.ndarray\n",
    "        The dataset to evaluate.\n",
    "    center : numpy.ndarray or list\n",
    "        The list of cluster centers.\n",
    "    distance : function\n",
    "        A function that computes the pairwise distance between two samples.\n",
    "        Default: Euclidean (l_2) distance\n",
    "    mean : bool\n",
    "        If True, returns the mean inertia.\n",
    "        Default: True\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    total_inertia : float\n",
    "       The mean inertia (if mean==True) or total inertia (if mean==False).\n",
    "\"\"\"\n",
    "    num_samples,_ = dataset.shape\n",
    "    total_inertia=0\n",
    "    if distance is None:\n",
    "        distance=lambda x,y: np.linalg.norm(x-y)**2\n",
    "    for i,sample in enumerate(dataset):\n",
    "        center = centers[labels[i]]\n",
    "        total_inertia += distance(sample,center)\n",
    "    \n",
    "    if mean:\n",
    "        return total_inertia/num_samples\n",
    "    else:\n",
    "        return total_inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute the **mean** inertia for each value of k on the training and validation datasets.\n",
    "1. For the training data, you can extract the *total* (**unnormalized**) inertia from the trained models via their respective ```inertia_```.\n",
    "2. For the validation models, you can utilize the function ```total_inertia``` provided below.\n",
    "\n",
    "### Comment:\n",
    "1. Given ```total-inertia``` has way to computer mean intertia, if we set mean to true. If that's the case, is that what you want us to use for training data as well?\n",
    "2. what do you mean by ***via their respective*** ```inertia_``` ?\n",
    "3. Mean inertia for train model, work, I think?\n",
    "4. But for the output for valid, the mean sse is coming the same\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 7,  3,  3, ...,  9, 13, 11])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inertia for:  2 Clusters is: 2336250.932170233\n",
      "The inertia for:  3 Clusters is: 1641197.1623104494\n",
      "The inertia for:  4 Clusters is: 1267215.461832596\n",
      "The inertia for:  5 Clusters is: 1006545.0258230997\n",
      "The inertia for:  6 Clusters is: 884104.1113676024\n",
      "The inertia for:  7 Clusters is: 766535.2582988844\n",
      "The inertia for:  8 Clusters is: 690753.6016601939\n",
      "The inertia for:  9 Clusters is: 630443.9907766646\n",
      "The inertia for:  10 Clusters is: 577790.8958285822\n",
      "The inertia for:  15 Clusters is: 419745.3334589625\n",
      "The inertia for:  20 Clusters is: 342673.29269367456\n"
     ]
    }
   ],
   "source": [
    "# dataset= forest_train_proj | centers = kmeans.cluster_centers_ | labels = kmeans.labels | mean=true\n",
    "# total_inertia(forest_train_proj,kmeans.cluster_centers_,kmeans.labels_,mean=True)\n",
    "\n",
    "mean_train = []\n",
    "num_samples,_ = forest_train_proj_scaled.shape\n",
    "for k in k_values:\n",
    "    km= KMeans(n_clusters=k, random_state=23508893).fit(forest_train_proj_scaled)\n",
    "    u = km.inertia_/num_samples\n",
    "    mean_train.append(u)\n",
    "    print(\"The inertia for: \", k, \"Clusters is:\", u)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#combine mean_valid and mean_train cell\n",
    "# dataset= forest_validation_proj | centers = kmeans.cluster_centers_ | mean=false\n",
    "# total_inertia(dataset,centers,labels,distance=None,mean=True)\n",
    "# mean_valid = []\n",
    "# for k in k_values:\n",
    "#     forest_valid_inertia =total_inertia(forest_validation_proj,kmeans.cluster_centers_, kmeans.labels_, mean=True, distance=None)\n",
    "#     km= KMeans(n_clusters=k).fit(forest_validation_proj)\n",
    "#     mean_valid.append(forest_valid_inertia)\n",
    "\n",
    "total_inertia(forest_validation_proj_scaled,kmeans.cluster_centers_, kmeans.labels_, mean=False, distance=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.xlabel('K')\n",
    "plt.ylabel('Sum of squared error')\n",
    "plt.plot(k_values, mean_train, color = 'black', label ='mean_train')\n",
    "# plt.plot(k_values, forest_valid_inertia, color = 'blue', label ='mean_valid')\n",
    "plt.legend(['mean_train', 'mean_valid'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the mean inertias for all values of $k$ and for both the training and validation sets on a single plot.\n",
    "1. Based on the elbow method, which value of $k$ should be chosen so that the model will generalize to new data?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "bb119707",
   "metadata": {},
   "source": [
    "# Part 2: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364bb3",
   "metadata": {},
   "source": [
    "In this part of the project, you will be utilizing a US database of crime and law enforcement statistics broken down by US Census communities. The **goal** will be to create *regression models* that predict *per capita violent crimes* (the response variable `ViolentCrimesPerPop`) for a given community based on these inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_column_names= ['state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize',\n",
    "                 'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29',\n",
    "                 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf',\n",
    "                 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap',\n",
    "                 'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov',\n",
    "                 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu',\n",
    "                 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr',\n",
    "                 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par',\n",
    "                 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent',\n",
    "                 'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8',\n",
    "                 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup',\n",
    "                 'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous',\n",
    "                 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded',\n",
    "                 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal',\n",
    "                 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc',\n",
    "                 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85',\n",
    "                 'PctSameCity85', 'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps',\n",
    "                 'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic',\n",
    "                 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp',\n",
    "                 'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', 'PolicAveOTWorked',\n",
    "                 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr',\n",
    "                 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop', 'ViolentCrimesPerPop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb673a",
   "metadata": {},
   "source": [
    "## 2.1 Load Data\n",
    "Set the variable `CRIME_FILE` to the **full path** to the forest cover dataset (**crime.csv**) on your system. Load the file into a dataframe (you may initialize the column names using the header list `crime_column_names`), then:\n",
    "1. Determine the number and types of features.\n",
    "2. Perform a **ShuffleSplit** of the data into training/validation/test sets, 60%/20%/20%. \n",
    "3. Split the **non-test data** (*training* + *validation* data) into **5 folds** for cross-validation purposes.\n",
    "4. Perform any necessary preprocessing on dataset. This may include:\n",
    "  * determining if any features should be dropped;\n",
    "  * handling missing data, through imputation and/or complete case analysis. If you **perform imputation on numerical values**, please use **median** imputation.\n",
    "\n",
    "**Please note!** The Scikit-learn function `SimpleImputer` does not work as expected when the feature to impute is numerical but the missing values are not. One way to solve this is to first replace the missing values with NaN values (e.g., `np.nan`) using the Pandas Dataframe method `replace` [(documentation)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# crime_file = f'C:/Users/tanzi/CS Lang IDE/PycharmProjects/Project-2/UTF-8_crime.csv'\n",
    "crime_file = f'C:/Users/tanzi/CS Lang IDE/PycharmProjects/Jonathan/Project2/Project-2/UTF-8_crime.csv'\n",
    "crime_data = pd.read_csv(crime_file, names=crime_column_names)\n",
    "crime_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "num_features = len(crime_column_names)\n",
    "print('Number of features: ', num_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Determine nature of object types\n",
    "crime_cat_columns=dict()\n",
    "crime_cat_columns_idx=dict()\n",
    "crime_num_columns_idx=dict()\n",
    "for col in crime_column_names:\n",
    "    if crime_data[col].dtype == 'object':\n",
    "        crime_cat_columns[col]=crime_data[col].unique()\n",
    "        crime_cat_columns_idx[col]=crime_data.columns.get_loc(col)\n",
    "        print(col,\":\",crime_cat_columns[col])\n",
    "    elif np.issubdtype(crime_data.at[0,col], np.number):\n",
    "        crime_num_columns_idx[col]=crime_data.columns.get_loc(col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removal of redundact data\n",
    "# Comment:\n",
    "1. 1st approach: debating between state and county vs commmunityname. I highly think community name should be removed, since it's a string.\n",
    "but the reason why I did not remove it because as the description talked about *given community*, which is the input. or is community enough?\n",
    "2. 2nd apporach:I am assuming community mean number of people are living in communityname. And that's why I was thinking of encoding communityname.\n",
    "3. Question 1 and 2 got my answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#let's drop county as it is redundant\n",
    "crime_data.drop('county', axis=1, inplace=True)\n",
    "crime_column_names.remove('county')\n",
    "#need to remap indices\n",
    "crime_cat_columns = dict()\n",
    "crime_cat_columns_idx = dict()\n",
    "crime_num_columns_idx = []\n",
    "for col in crime_column_names:\n",
    "    if crime_data[col].dtype == 'object':\n",
    "        crime_cat_columns[col] = crime_data[col].unique()\n",
    "        crime_cat_columns_idx[col] = crime_data.columns.get_loc(col)\n",
    "        print(col, \":\", crime_cat_columns[col])\n",
    "    elif np.issubdtype(crime_data.at[0, col], np.number):\n",
    "        crime_num_columns_idx.append(crime_data.columns.get_loc(col))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling missing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#In order to handle missing data, Replace ? with Nan\n",
    "crime_data.replace('?', np.nan, inplace= True)\n",
    "crime_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_data[crime_data.columns[crime_data.isnull().any()]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fillNa values with median imputation\n",
    "crime_data.fillna(crime_data.median(),inplace=True)\n",
    "# #Convert community object into int64\n",
    "# crime_data['community']=crime_data['community'].astype('int64')\n",
    "crime_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encode the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode the object type in crime_data\n",
    "\n",
    "#\n",
    "X_data_ =\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split_test = StratifiedShuffleSplit(n_splits=1,test_size=0.2, random_state=23508893)\n",
    "for train_index, test_index in split_test.split(X_data_enc, Y_data_enc):\n",
    "    X_train_proj_full = X_data_enc[train_index,:]\n",
    "    X_test_proj = X_data_enc[test_index,:]\n",
    "    Y_train_proj_full = Y_data_enc[train_index]\n",
    "    Y_test_proj = Y_data_enc[test_index]\n",
    "\n",
    "split_validation = StratifiedShuffleSplit(n_splits=1,test_size=0.25, random_state=23508893)\n",
    "for train_index, validation_index in split_validation.split(X_train_proj_full, Y_train_proj_full):\n",
    "    X_train_proj = X_train_proj_full[train_index,:]\n",
    "    X_validation_proj = X_train_proj_full[validation_index,:]\n",
    "    Y_train_proj = Y_train_proj_full[train_index]\n",
    "    Y_validation_proj = Y_train_proj_full[validation_index]\n",
    "\n",
    "#reshape labels for sklearn\n",
    "Y_train_proj_full =  Y_train_proj_full.reshape((Y_train_proj_full.shape[0],))\n",
    "Y_train_proj = Y_train_proj.reshape((Y_train_proj.shape[0],))\n",
    "Y_validation_proj= Y_validation_proj.reshape((Y_validation_proj.shape[0],))\n",
    "Y_test_proj = Y_test_proj.reshape((Y_test_proj.shape[0],))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Dataset\\t\\tPercent\")\n",
    "print(\"Training\\t\" + str(np.sum(crime_train_proj) / len(crime_train_proj)))\n",
    "print(\"Validation\\t\" + str(np.sum(crime_validation_proj) / len(crime_validation_proj)))\n",
    "print(\"Test\\t\\t\" + str(np.sum(crime_test_proj) / len(crime_test_proj)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peformance metric functions\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "import numpy as np\n",
    "\n",
    "#A list of keys for the dictionary returned by p1_metrics\n",
    "metric_keys = ['mse','mae','r2']\n",
    "\n",
    "def p2_metrics(y_true,y_pred,negation=False):\n",
    "    if negation:\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "    return {\n",
    "        'mse': sign*mean_squared_error(y_true,y_pred),\n",
    "        'mae': sign*mean_absolute_error(y_true,y_pred),\n",
    "        'r2': sign*r2_score(y_true,y_pred)}\n",
    "\n",
    "#This wrapper can be used to return multiple performance metrics during cross-validation\n",
    "def p2_metrics_scorer(clf,X,y_true):\n",
    "    y_pred=clf.predict(X)\n",
    "    return p2_metrics(y_true,y_pred,negation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c4474",
   "metadata": {},
   "source": [
    "## 2.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b802a9fb",
   "metadata": {},
   "source": [
    "1. Construct a linear model using Scikit-learn's `LinearRegression` method with default parameters.\n",
    "2. Report the following performance metrics on the **training and validation sets**:\n",
    "    *Mean Squared Error*, *Mean Absolute Error*, and the *Coefficient of Determination ($r^2$)*.\n",
    "    \n",
    "    You can use the function `p2_metrics` for this purpose. Is this model underfitting the data? Is so, why?\n",
    "3. Report the weights (coefficients) of the linear model and their associated features in ascending order.\n",
    "\n",
    "    Larger weights indicate that their corresponding features have more influence in the model. Moreover, negative weights correspond to variables having negative correlation with the response variable, and vice versa.\n",
    "    \n",
    "    Using this interpretation, describe the most significant features and their correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d09ba6",
   "metadata": {},
   "source": [
    "## 2.3 Linear Regression and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71808a",
   "metadata": {},
   "source": [
    "1. Perform principal component analysis on the **training data**. You may use Scikit-learn's `PCA` function for this, which **automatically centers** the data prior to PCA. Using PCA, *choose the number of components* for which the total explained variance is $\\ge 99\\%$, and report this.\n",
    "2. After determining the correct number of components, apply the PCA transformation to the **validation** and **test** sets.\n",
    "3. Create another model via `LinearRegression` but using the data transformed by PCA\n",
    "Construct a linear model using Scikit-learn's `LinearRegression` method with default parameters.\n",
    "4. Report the same performance metrics as in 2.2 on the **validation set**. How does the model's performance compare to that of the model in 2.2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09279f2",
   "metadata": {},
   "source": [
    "## 2.4 LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d321fa",
   "metadata": {},
   "source": [
    "Utilizing *cross-validation* you will Construct an $\\ell_1$-regularized linear model using Scikit-learn's `LASSO`:\n",
    "1. Using `GridSearchCV`, determine the best choice of the hyperparameter $\\alpha$ out of values in the list `alphas` below.\n",
    "2. Report the time required to perform cross-validation via `GridSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameter. You may use the function `collate_ht_results` for this purpose.\n",
    "3. Report the weights (coefficients) of the LASSO model and their associated features in ascending order. Note that LASSO attempts to set as many weights to zero in order to create a more parsimonious model while still maintaining regression performance. How many weights are non-zero?\n",
    " \n",
    "### Please Read!\n",
    "There are a few parameters for the `GridSearchCV` and `RandomizedSearchCV` functions that should be set:\n",
    "- `scoring` - This controls the strategy to evaluate the performance of the cross-validated model on the test set, set it to `p2_metrics_scorer`.\n",
    "- `refit` - This will refit an estimator using the best found parameters on the whole dataset, set it to `\"mse\"`\n",
    "- `cv` - This will enable you to reuse your CV splits created in Part 2.1\n",
    "    `n_jobs` - Number of jobs to run in parallel, if you have more than one core on your device (you should), set this to as many as you'd like to use, or to `-1` if you want to use all available cores.\n",
    "- `return_train_score` - Setting this to `False` will reduce computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283aac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(1,-3,50)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizes model performance results produced during hyperparameter tuning\n",
    "def collate_ht_results(ht_results,metric_keys=metric_keys,display=True):\n",
    "    ht_stats=dict()\n",
    "    for metric in metric_keys:\n",
    "        ht_stats[metric+\"_mean\"] = ht_results.cv_results_[\"mean_test_\"+metric][ht_results.best_index_]\n",
    "        ht_stats[metric+\"_std\"] = metric_std = ht_results.cv_results_[\"std_test_\"+metric][ht_results.best_index_]\n",
    "        if display:\n",
    "            print(\"test_\"+metric,ht_stats[metric+\"_mean\"],\"(\"+str(ht_stats[metric+\"_std\"])+\")\")\n",
    "    return ht_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1aff9",
   "metadata": {},
   "source": [
    "## 2.5 Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26ebb4",
   "metadata": {},
   "source": [
    "Utilizing *cross-validation* you will construct an MLP regression model using Scikit-learn's `MLPRegressor`:\n",
    "1. Using `GridSearchCV`, determine the best choice of hyperparameters out of the following possible values:\n",
    "- *Number of hidden layers*: [1, 2, 3]\n",
    "- *Number of neurons per layer*: [10, 20, 50]\n",
    "- *Learning rate*: [1e-5, 1e-4, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "\n",
    "2. Report the time required to perform cross-validation via `GridSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameter. You may use the function `collate_ht_results` for this purpose.\n",
    "\n",
    " \n",
    "### Please Read!\n",
    "In addition to utilizing the same `GridSearchCV` parameters as in 2.5, the `MLPRegressor` function should have the following parameters set:\n",
    "- `max_iter` -  This controls the maximum number of rounds of backpropagation/gradient descent; set it to 10,000.\n",
    "- `early_stopping` - This will reserve a portion of the training data tha can be used to evaluate convergence progress in order to stop training early; set it to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f4f9a",
   "metadata": {},
   "source": [
    "## 2.6 Final Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b7dcb",
   "metadata": {},
   "source": [
    "1. Using the full training set (**training + validation**), train *two* linear regression models, one with and without PCA preprocessing, then apply them to the test set. For LASSO and MLP, you can utilize the best models found during cross-validation and just apply them to the test set.\n",
    "2. Create a bar chart of the three regression metrics for each model on the same plot.\n",
    "3. How do the models's performances compare? What do the metrics reveal about the dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
