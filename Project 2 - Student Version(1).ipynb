{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing jupyter\n"
     ]
    }
   ],
   "source": [
    "print('testing jupyter')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "46c91f27",
   "metadata": {},
   "source": [
    "# CSCI 381/780 (Fall 2022) - Project 2\n",
    "\n",
    "**Due Date: Monday, November 14 by 4 PM**\n",
    "\n",
    "## Description\n",
    "In this project you will construct machine learning models on two different real-world datasets using unsupervised learning and regression.\n",
    "\n",
    "## Instructions\n",
    "1. In this project you will write code to construct machine learning models and write responses to questions concerning the performance of said models. Please complete all sections below, adding new *Code* or *Markdown* cells as appropriate to answer the questions.\n",
    "2. There are many Scikit-learn functions that leverage randomness to generate results. For these functions, a pseudorandom generator can be initialized using a seed value by passing the parameter `random_state=XXX`, where `XXX` is some number between 1 and 2^31-1. For each of these functions, **you will utilize your CUNY ID number** to initialize the function. Functions include:\n",
    "- `ShuffleSplit`\n",
    "- `KFold`\n",
    "- `KMeans`\n",
    "- `GridSearchCV`\n",
    "- `Lasso`\n",
    "- `MLPRegressor`\n",
    "3. You will **work independently** on the project. Please make use of the *Python Data Science Reference Materials* posted on Blackboard, or **come to office hours should you need further assistance**.\n",
    "4. You will submit a single Jupyter notebook containing all code and written responses via Blackboard by the due date listed above. \n",
    "\n",
    "## Grading\n",
    "\n",
    "### Running Code\n",
    "Your Jupyter notebook must be able to run from start to finish **without error**. Please turn any cell that contains scratch work or other non-executable items to *Raw*. **Notebooks that cannot run to completion will receive a grade of 0**.\n",
    "\n",
    "\n",
    "### Rubric\n",
    "\n",
    "|**Part**|1.1|1.2|2.1|2.2|2.3|2.4|2.5|2.6|**Total**|\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**%**|10|30|15|5|10|10|10|10|100|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c3272a",
   "metadata": {},
   "source": [
    "# Part 1: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a550163a",
   "metadata": {},
   "source": [
    "In this part of the project, you will be using data collected by the US Geological Survey and the US Forest Service. The data describes various cartographic and geologic features related to forest cover in US wilderness areas, with each sample representing a 30 x 30 meter cell.\n",
    "\n",
    "The **goal** will be to *identify clusters* that represent forest cover types, and using these clusters *extract relationships* between forest covers and the provided cartographic/geologic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fcfa560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "cover_column_names=[\"Elevation\",\"Aspect\",\"Slope\",\n",
    "              \"Horizontal_Distance_To_Hydrology\",\"Vertical_Distance_To_Hydrology\",\n",
    "              \"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\n",
    "              \"Hillshade_3pm\",\"Horizontal_Distance_To_Fire_Points\"]\n",
    "wilderness_cols =[\"Wilderness_\"+str(i) for i in range(4)]\n",
    "soil_col =[\"Soil_\"+str(i) for i in range(40)]\n",
    "cover_columns_names = cover_column_names + wilderness_cols + soil_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8124dc",
   "metadata": {},
   "source": [
    "## 1.1 Load Data\n",
    "Set the variable `COVER_FILE` to the **full path** to the forest cover dataset (**forest_cover_dataset.csv**) on your system. Load the file into a dataframe (you may initialize the column names using the header list `cover_column_names`), then:\n",
    "1. Determine the number and types of features.\n",
    "2. Perform a **ShuffleSplit** of the data into training/validation/test sets, 60%/20%/20%. \n",
    "3. **Center** the training/validation/test splits (fit on the training set, then transform the validation/test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "        Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n0            2825     265     17                               319   \n1            2719     198     34                               134   \n2            3146     152     14                               212   \n3            2882      18     18                                95   \n4            2912     349     17                               283   \n...           ...     ...    ...                               ...   \n280246       2919     224     18                                85   \n280247       2151     114     29                                42   \n280248       3008     221     14                               418   \n280249       2923      58     12                               134   \n280250       3294     198      6                               484   \n\n        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n0                                  108                             2298   \n1                                   76                             2352   \n2                                   41                              940   \n3                                   -3                              485   \n4                                   57                             2729   \n...                                ...                              ...   \n280246                              24                              451   \n280247                              29                              499   \n280248                              99                             5346   \n280249                              12                             2213   \n280250                              80                              849   \n\n        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n0                 174             245            209   \n1                 187             245            167   \n2                 237             239            130   \n3                 203             199            128   \n4                 187             210            160   \n...               ...             ...            ...   \n280246            193             254            193   \n280247            254             197             49   \n280248            202             253            185   \n280249            228             214            117   \n280250            218             245            163   \n\n        Horizontal_Distance_To_Fire_Points  ...  Soil_30  Soil_31  Soil_32  \\\n0                                      342  ...        0        0        1   \n1                                      693  ...        0        0        0   \n2                                     2007  ...        0        0        0   \n3                                      342  ...        1        0        0   \n4                                     2201  ...        0        0        0   \n...                                    ...  ...      ...      ...      ...   \n280246                                1436  ...        0        0        0   \n280247                                 731  ...        0        0        0   \n280248                                1300  ...        0        0        0   \n280249                                 424  ...        0        0        0   \n280250                                2704  ...        0        1        0   \n\n        Soil_33  Soil_34  Soil_35  Soil_36  Soil_37  Soil_38  Soil_39  \n0             0        0        0        0        0        0        0  \n1             0        0        0        0        0        0        0  \n2             0        0        0        0        0        0        0  \n3             0        0        0        0        0        0        0  \n4             0        0        0        0        0        0        0  \n...         ...      ...      ...      ...      ...      ...      ...  \n280246        0        0        0        0        0        0        0  \n280247        0        0        0        0        0        0        0  \n280248        0        0        0        0        0        0        0  \n280249        0        0        0        0        0        0        0  \n280250        0        0        0        0        0        0        0  \n\n[280251 rows x 54 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Elevation</th>\n      <th>Aspect</th>\n      <th>Slope</th>\n      <th>Horizontal_Distance_To_Hydrology</th>\n      <th>Vertical_Distance_To_Hydrology</th>\n      <th>Horizontal_Distance_To_Roadways</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Hillshade_3pm</th>\n      <th>Horizontal_Distance_To_Fire_Points</th>\n      <th>...</th>\n      <th>Soil_30</th>\n      <th>Soil_31</th>\n      <th>Soil_32</th>\n      <th>Soil_33</th>\n      <th>Soil_34</th>\n      <th>Soil_35</th>\n      <th>Soil_36</th>\n      <th>Soil_37</th>\n      <th>Soil_38</th>\n      <th>Soil_39</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2825</td>\n      <td>265</td>\n      <td>17</td>\n      <td>319</td>\n      <td>108</td>\n      <td>2298</td>\n      <td>174</td>\n      <td>245</td>\n      <td>209</td>\n      <td>342</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2719</td>\n      <td>198</td>\n      <td>34</td>\n      <td>134</td>\n      <td>76</td>\n      <td>2352</td>\n      <td>187</td>\n      <td>245</td>\n      <td>167</td>\n      <td>693</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3146</td>\n      <td>152</td>\n      <td>14</td>\n      <td>212</td>\n      <td>41</td>\n      <td>940</td>\n      <td>237</td>\n      <td>239</td>\n      <td>130</td>\n      <td>2007</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2882</td>\n      <td>18</td>\n      <td>18</td>\n      <td>95</td>\n      <td>-3</td>\n      <td>485</td>\n      <td>203</td>\n      <td>199</td>\n      <td>128</td>\n      <td>342</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2912</td>\n      <td>349</td>\n      <td>17</td>\n      <td>283</td>\n      <td>57</td>\n      <td>2729</td>\n      <td>187</td>\n      <td>210</td>\n      <td>160</td>\n      <td>2201</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>280246</th>\n      <td>2919</td>\n      <td>224</td>\n      <td>18</td>\n      <td>85</td>\n      <td>24</td>\n      <td>451</td>\n      <td>193</td>\n      <td>254</td>\n      <td>193</td>\n      <td>1436</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>280247</th>\n      <td>2151</td>\n      <td>114</td>\n      <td>29</td>\n      <td>42</td>\n      <td>29</td>\n      <td>499</td>\n      <td>254</td>\n      <td>197</td>\n      <td>49</td>\n      <td>731</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>280248</th>\n      <td>3008</td>\n      <td>221</td>\n      <td>14</td>\n      <td>418</td>\n      <td>99</td>\n      <td>5346</td>\n      <td>202</td>\n      <td>253</td>\n      <td>185</td>\n      <td>1300</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>280249</th>\n      <td>2923</td>\n      <td>58</td>\n      <td>12</td>\n      <td>134</td>\n      <td>12</td>\n      <td>2213</td>\n      <td>228</td>\n      <td>214</td>\n      <td>117</td>\n      <td>424</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>280250</th>\n      <td>3294</td>\n      <td>198</td>\n      <td>6</td>\n      <td>484</td>\n      <td>80</td>\n      <td>849</td>\n      <td>218</td>\n      <td>245</td>\n      <td>163</td>\n      <td>2704</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>280251 rows × 54 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cover_file = f'C:/Users/tanzi/CS Lang IDE/PycharmProjects/Project-2/UTF-8_forest_cover_dataset.csv'\n",
    "cover_forestD = pd.read_csv(cover_file, names=cover_columns_names)\n",
    "cover_forestD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  54\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of features\n",
    "num_features = len(cover_columns_names)\n",
    "print('Number of features: ', num_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elevation : [2825 2719 3146 ... 3626 3541 3607]\n",
      "Aspect : [265 198 152  18 349 202 102  90   9 117 315  40 110   3 359 106 108 150\n",
      "  32 347 127  62 123 176  53 100 180 166 189 243 294  72 112  22  45 274\n",
      "  60 344  28 299  65  34 169  73  43  82 262 232  83  37 329  49 269  66\n",
      " 140 133  33  61 350  63  74 326 193 225 226 342 318 276 316 167 303  55\n",
      " 131  69 251  19 220  52 135  11 270  26  14 357  59  42  51 158 204 138\n",
      " 356  78  12 306 165 238 235 352 219 307 119 311 351  36  95  25 163 162\n",
      " 272  17 340 113  41 348   0 324  35  68  13  20 334 343 304  85  96 146\n",
      "  99 143 301  79  98 336 196  21 101 116 170 317 245  38  92 111 211 354\n",
      " 121   8 249 157 277  31   4 181 322 105 288 332 185 187 241 199 172 289\n",
      " 207 281 291 222 107  93 302 195  56 247 314 327   7 287 263 333 309 209\n",
      " 148 331   1 266   5  91  15 268  16 145 184 254 194  71 337 183 346 149\n",
      "  80 285  58  57  89 118 330  50 125 296 182  27 231 248  54  70  97 103\n",
      " 156 147  76  23  67 338 256 174 323 358 177 267 300 215  30 206 203  44\n",
      " 252 171 134  39 155 328   6  87 234 122 128  64 139 295 130 228  81 159\n",
      " 218 286 240 173 257  29 141 104  94 339 205 144 221 164 305 335 341 153\n",
      "   2  47 197 129 308 137 253 216 178 201  48 142 284 208 271 258 297 132\n",
      "  75  84 154 109 310 321 217 292  88 260 313 278  10 190 345  24 114 160\n",
      " 242 120 325  86 320 213 161 136 175 293 282 210 261 230 259 312 233 319\n",
      " 236 353 168 255 355 279 214 124 229 264 191 280 192 250 273  77 227 223\n",
      " 283 275 290 126 246 151 179 237 244 115 188 239 298  46 200 212 186 224\n",
      " 360]\n",
      "Slope : [17 34 14 18 11 12  6  4 15 21  9 25 19 10  8 16  1 20 24  5  2 13  3  7\n",
      " 33 23 38 28 29 22 31 30 35 26 36 27 32 37  0 39 40 48 42 43 45 46 41 47\n",
      " 49 44 57 50 59 53 52 51 61 65 54 64 56 55 60 66 63]\n",
      "Horizontal_Distance_To_Hydrology : [ 319  134  212   95  283  192  342  330  255  234   30    0  726   60\n",
      "  630  295  541   85  216  400   42  361  351  210  162  362  190  458\n",
      "  153   90   67  277  331  180  124  120  268  607  424  323  828  150\n",
      "  564  480  570  228  390  677  379  450  631  391  170  258  270  272\n",
      "  601  218  636  256  297  324  108  240  547  540  360  309  301  700\n",
      "  175  577  242  306  633  247  902  430  655  335  285  182  513  466\n",
      "  201  420  499  573  382  495  313  127  600  395  750  300  716  417\n",
      "  467  402  780  722  376  534  484  794  781  474  876  339  624  514\n",
      "  618  497  819 1170  408  511  979  509  671  595  433  591  800  768\n",
      "  366  721  552  900  350  642  582  693  371  875  446  849  870  531\n",
      "  799  955  459  616  524  384  443  702  757  560  684  510  404  469\n",
      "  537  576  641  503  742  930  732  551  765  787  741  553  457  437\n",
      "  816  658  664  984  969  418  589  604  959  942  690  569  488  960\n",
      "  571  612  957  920  663  454 1271  792  451  698  660  759  485  365\n",
      "  895  713  810  470  769  516  711  845  481  426  566  836  691  745\n",
      "  626 1092  518  872  720  830  782  532  659  866  598  674  421  824\n",
      "  649  708  646  666  543  834  771  743  558  638  859 1068  592 1034\n",
      "  774  755  886  492  807  865  841  735  785 1006  840  949  680  594\n",
      "  931  854 1315  706 1084  764  751  603  987  908  731  752  977  525\n",
      "  864  990  760  725  685  892  797 1154  703  661  805 1082  679  994\n",
      " 1131  579 1061 1259  730  912  778 1213  808  972  953  644  779  858\n",
      "  962  888  899  997  924  738 1142  911 1198  914 1116  933  696  976\n",
      " 1275 1024 1260 1087  850  811  812  999  837  947  789 1065  832  878\n",
      "  767 1080  891 1273  966  806  860  983 1121  815  890 1265  853  648\n",
      " 1129 1300 1310  932 1044 1050 1113  907 1182 1040  973  883  904  993\n",
      " 1073  874 1063 1020 1203 1048 1045 1055 1304 1074 1077 1071 1205 1008\n",
      "  871 1230 1114 1302  939  847  964 1120 1018 1124  937  882 1103 1158\n",
      "  793  842  916 1329 1180 1026  918 1100  934 1041 1282 1099 1025  927\n",
      " 1012 1277  938 1245 1115 1032 1128 1022 1194 1027 1053  981 1110 1101\n",
      " 1015 1150 1224 1060 1062 1106 1072 1237 1054 1047 1233 1172  825 1052\n",
      " 1195 1201 1221 1243 1216  926 1167 1200  967 1019  940 1001  992 1138\n",
      " 1342 1107 1148 1003 1261 1036 1031 1159  968 1318 1383 1140 1126 1208\n",
      " 1288 1127 1253 1235 1231 1222 1226 1146 1215 1165 1199 1206 1240 1091\n",
      " 1332 1266 1316 1075 1155 1104 1250 1269 1090 1218 1348 1123 1234 1328\n",
      " 1033 1057 1112 1130 1173 1187 1248 1283 1358 1276 1149 1064 1136 1184\n",
      " 1254 1183 1177 1290 1209 1189 1361 1179 1168 1307 1371 1211 1292 1134\n",
      " 1236 1262 1319 1289 1239 1095 1343 1397 1190 1295 1249 1354 1176 1324\n",
      " 1320 1390 1188 1368 1301 1191 1144 1207]\n",
      "Vertical_Distance_To_Hydrology : [ 108   76   41   -3   57   36   47   46   26  -12   72    5    0   78\n",
      "   22   45   62    7   18   34   -8   13    1   40    4  171   19   11\n",
      "   -2   49  -22   70   58   -9   63  -21   32   44   51   53    2   15\n",
      "   16   17  150   10    9  -33  163    6  103   23  135   25  180   82\n",
      "    8   33  109   35   24   12   28  129  -14   86   92   59    3   97\n",
      " -145   14   30  -37   54   66   31   68   -1   42  213   84   77  128\n",
      "   73   -5  134   56   69  131   48   -4  115   21  206   39  -11   90\n",
      "  -51  177  -23   52  -18  123  104   88  101  116  236   38   50  -27\n",
      "  193  105   96  148   91   37   -7   89  111   83   79   67   43  158\n",
      "  -47  200  140  124   61   65  190   85   55  100   75  102  154  122\n",
      "  281  118  310  165  175   81   27   95  120  166   29  205  138  300\n",
      "   20  -13  146  -10  -16   94  132  239  -44  202  -31  -19  182  -30\n",
      "   60  143  302 -139  139   80  110   74  194  191   98  -43   64   71\n",
      "  174  151  -17  121  183  136  -38  117  203  289  -60   -6  107  -46\n",
      "  160  262  -56  181  114  -62  162  212  155  -65   99  125  -88  215\n",
      "  296  -25  238  172   93  219  240  -35  303  127  168  113  228  144\n",
      "  290  112  119  218  164  188  185  133  224  159  210  -48  225  227\n",
      "  -32  -15  141  229  147  -34  263  -39  189  126  161  170  169  197\n",
      "  -80   87  570  209  153  142  187  -45  -50  186  -58  -57  157  195\n",
      "  207  223  -24  -41  288  192  -59  -93  279  156  244  -42  199  -49\n",
      "  221  217  149  137 -106 -124  305  415  145  -29  -28  -55  -26  252\n",
      "  -78  -75  173  258  246  370  167  368  211  286  275  247  -36  249\n",
      " -111  214  179  233  311  130  198  276  106  253  178  313 -101  152\n",
      "  266  -68 -114 -130  -86  -74  -20  204  176  235 -119  -77 -104  230\n",
      "  314  237  -72  -61  222  270  -52  363  232  216  331  366  255  264\n",
      "  201  184  -90 -113  -82  401  196  250 -116  341  364  394  265  304\n",
      "  297  241  -66  278  361  283  242  231 -125  -76  -64  257  259  343\n",
      "  -97  220  251  -40  -85  -54  379  285  271  269  373 -121  306  329\n",
      "  337  322  -73  307  330  408  -53  -71  291  -83  -99 -105  292  245\n",
      " -110  234  414  325  226  353  268  -98  316  398 -102  318 -123  248\n",
      "  443  345 -107  254  -67  375  267  277  -96  256  -81  261  295  260\n",
      "  334  274  550  -69  -94  333  367  354  208  243  319  282 -133  346\n",
      " -149  -70  -91 -108  402  336  -95 -103  356  358  273  -89  453 -100\n",
      "  -63 -112 -118  312  391 -166  -92  317 -109  324  293  389 -143  419\n",
      "  294  -79  338  272  327  284  359  308 -140  287  309  543  -84 -137\n",
      " -155  377  381 -115  355  493  386  371 -134  348 -159  421 -117  347\n",
      "  554  362  395  360  466 -132  365  523  335 -129  332  369 -128  323\n",
      "  428  597 -131  280  -87  376 -136  301  400  512  328  351  528  446\n",
      "  383  350  298 -122  357  315  385  326 -135  568  478  349  320  299\n",
      "  378  393  407  340  344  396  342  517 -126  434  557  390 -120  321\n",
      "  339  491 -152 -150  485  537  374  405  427 -146  461  496  413  479\n",
      "  372  585  539  382  454  392  420  397  403  352  526  506 -153  380\n",
      "  592  561  565  589 -127 -141  571 -144  411  549  455  431  524  527\n",
      "  595  514  492  410  387  544  388  409  547  432  590  573  463  536\n",
      " -157 -151  598 -138  507  584  540  552  586  384 -173  578  406  404\n",
      "  417  462 -156  508 -163  582  504  399]\n",
      "Horizontal_Distance_To_Roadways : [2298 2352  940 ... 6805 6788 6409]\n",
      "Hillshade_9am : [174 187 237 203 215 240 230 207 227 178 216 248 214 206 252 245 239 212\n",
      " 201 247 232 224 226 238 223 221 219 246 211 194 197 231 202 234 241 220\n",
      " 165 222 160 218 208 225 184 229 199 190 145 166 243 200 210 249 213 217\n",
      " 204 196 180 185 244 250 170 135 183 209 143 114 179 191 251 147 176 181\n",
      " 228 235 126 182 242 124 195 162 205 172 146 253 132 167 198 188 125 161\n",
      " 236 107 171 163 155 169 141 159 177 233 175 192 149 189 153 144 137 164\n",
      " 186 168 193 138 127 254  94 173 157 129 151 120  90 156 150 140 158 134\n",
      " 148 102 112 154 142  93 128 139  87 130 136 152 122 117 121 131  88  75\n",
      " 104 116  70 115  99 133 109  92 106 103 111 123 100 108  96 119  85 110\n",
      " 105  54  73 113  52  76  89 118   0  67  95  97  74  79  80  98  91  78\n",
      "  71  64 101  84  83  56  65  82  81  86  77  46  66  68  36  59  72  69\n",
      "  61  60  57  55  62]\n",
      "Hillshade_Noon : [245 239 199 210 250 224 230 217 236 226 188 216 232 223 201 221 237 196\n",
      " 225 204 243 218 244 251 222 191 190 246 219 227 180 233 202 207 229 228\n",
      " 242 247 206 187 205 252 253 220 231 212 248 208 213 240 200 235 194 150\n",
      " 167 249 182 161 176 203 168 234 211 215 173 195 238 175 214 209 192 197\n",
      " 254 174 198 241 146 170 160 145 184 159 189 172 193 164 163 186 152 179\n",
      " 185 171 162 144 183 178 181 177 169 147 157 141 148 153 158 136 154 156\n",
      " 166 155 129 126 108 135 138 139 149 125 104 165 134 131 133 137  96 132\n",
      " 122 151 130 112 143  98   0 142 117 140 120 107  63 124 121 123 119 116\n",
      "  92  76 111  85 128  99  87 101 114  42 106  91 113 100 115  71 127 103\n",
      "  30 109  74 118  80 105 102 110  97  81  78  93  88  53  64  95  40  90]\n",
      "Hillshade_3pm : [209 167 130 128 160 168 111 134 146 143 188  98  89 155  62 100 122 113\n",
      " 162  99 154 126 116 156 139 159 169 197 144 109  95 150 163 103 195 141\n",
      " 110  90 184 108 133 182 218 121  91 153 104 140 137 136 124  64 112 164\n",
      " 185 189 179 187 200 201 127  97 114 107 174  87 165 148  65 172  94  84\n",
      " 175  59  18 186  69  88 208 157  58 138 241 145 123  78 152 181  92 105\n",
      " 147 149 177 166 191 101 132 125 102 192 151 221  83 142 194 212 135 131\n",
      " 203 225 115 158  17 196  72 180  81 170 206 171 202  80 204  67 224  53\n",
      " 173  42 217 207 119 161 220 183 120 129 117 211  57 236  76 198  74 118\n",
      " 205  46 210  79  93  56  71  43  77 229  85 199 106 176 178  31  48 213\n",
      "  96  21 222   2  41 242  73 223  47  40 214  55 193   9  37  14  75  63\n",
      "  52  34  27  60 190 232 227 244  45 216  86  61  51 237 249  82 219 228\n",
      " 233  16 226  32 240 234  38  68 231  70  44  12  30  22  50  29 239  28\n",
      " 215  66  36   0  20  23 235   4  49 230  13 248  54 246  11 243   5  39\n",
      " 238  25  33   6  26  24  35 247 245   8  15 254  19   1 251 252  10 250\n",
      "   7   3 253]\n",
      "Horizontal_Distance_To_Fire_Points : [ 342  693 2007 ... 6760 6203 4880]\n",
      "Wilderness_0 : [0 1]\n",
      "Wilderness_1 : [0 1]\n",
      "Wilderness_2 : [1 0]\n",
      "Wilderness_3 : [0 1]\n",
      "Soil_0 : [0 1]\n",
      "Soil_1 : [0 1]\n",
      "Soil_2 : [0 1]\n",
      "Soil_3 : [0 1]\n",
      "Soil_4 : [0 1]\n",
      "Soil_5 : [0 1]\n",
      "Soil_6 : [0 1]\n",
      "Soil_7 : [0 1]\n",
      "Soil_8 : [0 1]\n",
      "Soil_9 : [0 1]\n",
      "Soil_10 : [0 1]\n",
      "Soil_11 : [0 1]\n",
      "Soil_12 : [0 1]\n",
      "Soil_13 : [0 1]\n",
      "Soil_14 : [0]\n",
      "Soil_15 : [0 1]\n",
      "Soil_16 : [0 1]\n",
      "Soil_17 : [0 1]\n",
      "Soil_18 : [0 1]\n",
      "Soil_19 : [0 1]\n",
      "Soil_20 : [0 1]\n",
      "Soil_21 : [0 1]\n",
      "Soil_22 : [0 1]\n",
      "Soil_23 : [0 1]\n",
      "Soil_24 : [0 1]\n",
      "Soil_25 : [0 1]\n",
      "Soil_26 : [0 1]\n",
      "Soil_27 : [0 1]\n",
      "Soil_28 : [0 1]\n",
      "Soil_29 : [0 1]\n",
      "Soil_30 : [0 1]\n",
      "Soil_31 : [0 1]\n",
      "Soil_32 : [1 0]\n",
      "Soil_33 : [0 1]\n",
      "Soil_34 : [0 1]\n",
      "Soil_35 : [0 1]\n",
      "Soil_36 : [0]\n",
      "Soil_37 : [0 1]\n",
      "Soil_38 : [0 1]\n",
      "Soil_39 : [0 1]\n"
     ]
    }
   ],
   "source": [
    "#change the variable name\n",
    "\n",
    "#Determine nature of object types\n",
    "census_cat_columns=dict()\n",
    "census_cat_columns_idx=dict()\n",
    "census_num_columns_idx=dict()\n",
    "for col in cover_columns_names:\n",
    "    if cover_forestD[col].dtype == 'int64':\n",
    "        census_cat_columns[col]=cover_forestD[col].unique()\n",
    "        census_cat_columns_idx[col]=cover_forestD.columns.get_loc(col)\n",
    "        print(col,\":\",census_cat_columns[col])\n",
    "    elif np.issubdtype(cover_forestD.at[0,col], np.number):\n",
    "        census_num_columns_idx[col]=cover_forestD.columns.get_loc(col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a9330116",
   "metadata": {},
   "source": [
    "## 1.2 $k$-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66349c13",
   "metadata": {},
   "source": [
    "1. Cluster the *training* dataset using ```KMeans``` from Scikit-learn using the values for $k$ below. You can utilize the default implementation in Scikit-learn, which is $k$-means$++$. Construct models for each value of $k$ specified in the ``k_values`` array below.\n",
    "2. Apply the trained models to the validation dataset.\n",
    "3. Compute the **mean** inertia for each value of $k$ on the training and validation datasets. For the training data, you can extract the *total* (**unnormalized**) inertia from the trained models via their respective ```inertia_```. For the validation models, you can utilize the function ```total_inertia``` provided below.\n",
    "4. Plot the mean inertias for all values of $k$ and for both the training and validation sets on a single plot. Based on the elbow method, which value of $k$ should be chosen so that the model will generalize to new data?\n",
    "5. The *Silhouette Coefficient* is another cluster performance metric that combines intra-cluster distance with inter-cluster distances from clusters in close proximity to each other. Utilize Scikit-learn's ```silhouette_score``` function [(documentation)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) to compute the silhouette coefficient for all values of $k$ on the *validation* set. Plot these values and determine via the elbow method which value of $k$ should be chosen. How does this value compare to the one chosen using mean inertia?\n",
    "6. Using whichever value of $k$ you think is the best based on steps 4) and 5), create a plot for each feature that contains [boxplots](https://matplotlib.org/stable/gallery/statistics/boxplot_demo.html#sphx-glr-gallery-statistics-boxplot-demo-py) of that feature for each cluster. Based upon these plots, determine which features you think are meaningful in discriminating between the clusters, as well as any other trends you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf7aaa",
   "metadata": {},
   "source": [
    "### Please Read!\n",
    "You may benefit from parallelizing the creation of the $k$-means models by using the Python package ```joblib```. I've included a code template below that you can customize. For additional details, please refer to [joblib's documentation](https://joblib.readthedocs.io/en/latest/parallel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69c59675",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values=list(range(2,11))+[15,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd723091",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [51], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjoblib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Parallel, delayed, parallel_backend\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloky\u001B[39m\u001B[38;5;124m\"\u001B[39m, inner_max_num_threads\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m----> 4\u001B[0m     results \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)(delayed(func)(x, y) \u001B[38;5;28;01mfor\u001B[39;00m x, y \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdata\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed, parallel_backend\n",
    "\n",
    "with parallel_backend(\"loky\", inner_max_num_threads=2):\n",
    "    results = Parallel(n_jobs=4)(delayed(func)(x, y) for x, y in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_inertia(dataset,centers,labels,distance=None,mean=True):\n",
    "    \"\"\"\n",
    "    Computes the total inertia (the intracluster variance) given a dataset and the cluster centers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : numpy.ndarray\n",
    "        The dataset to evaluate.\n",
    "    center : numpy.ndarray or list\n",
    "        The list of cluster centers.\n",
    "    distance : function\n",
    "        A function that computes the pairwise distance between two samples.\n",
    "        Default: Euclidean (l_2) distance\n",
    "    mean : bool\n",
    "        If True, returns the mean inertia.\n",
    "        Default: True\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    total_inertia : float\n",
    "       The mean inertia (if mean==True) or total inertia (if mean==False).\n",
    "\"\"\"\n",
    "    num_samples,_ = dataset.shape\n",
    "    total_inertia=0\n",
    "    if distance is None:\n",
    "        distance=lambda x,y: np.linalg.norm(x-y)**2\n",
    "    for i,sample in enumerate(dataset):\n",
    "        center = centers[labels[i]]\n",
    "        total_inertia += distance(sample,center)\n",
    "    \n",
    "    if mean:\n",
    "        return total_inertia/num_samples\n",
    "    else:\n",
    "        return total_inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb119707",
   "metadata": {},
   "source": [
    "# Part 2: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41364bb3",
   "metadata": {},
   "source": [
    "In this part of the project, you will be utilizing a US database of crime and law enforcement statistics broken down by US Census communities. The **goal** will be to create *regression models* that predict *per capita violent crimes* (the response variable `ViolentCrimesPerPop`) for a given community based on these inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_column_names= ['state', 'county', 'community', 'communityname', 'fold', 'population', 'householdsize',\n",
    "                 'racepctblack', 'racePctWhite', 'racePctAsian', 'racePctHisp', 'agePct12t21', 'agePct12t29',\n",
    "                 'agePct16t24', 'agePct65up', 'numbUrban', 'pctUrban', 'medIncome', 'pctWWage', 'pctWFarmSelf',\n",
    "                 'pctWInvInc', 'pctWSocSec', 'pctWPubAsst', 'pctWRetire', 'medFamInc', 'perCapInc', 'whitePerCap',\n",
    "                 'blackPerCap', 'indianPerCap', 'AsianPerCap', 'OtherPerCap', 'HispPerCap', 'NumUnderPov', 'PctPopUnderPov',\n",
    "                 'PctLess9thGrade', 'PctNotHSGrad', 'PctBSorMore', 'PctUnemployed', 'PctEmploy', 'PctEmplManu',\n",
    "                 'PctEmplProfServ', 'PctOccupManu', 'PctOccupMgmtProf', 'MalePctDivorce', 'MalePctNevMarr',\n",
    "                 'FemalePctDiv', 'TotalPctDiv', 'PersPerFam', 'PctFam2Par', 'PctKids2Par', 'PctYoungKids2Par',\n",
    "                 'PctTeen2Par', 'PctWorkMomYoungKids', 'PctWorkMom', 'NumIlleg', 'PctIlleg', 'NumImmig', 'PctImmigRecent',\n",
    "                 'PctImmigRec5', 'PctImmigRec8', 'PctImmigRec10', 'PctRecentImmig', 'PctRecImmig5', 'PctRecImmig8',\n",
    "                 'PctRecImmig10', 'PctSpeakEnglOnly', 'PctNotSpeakEnglWell', 'PctLargHouseFam', 'PctLargHouseOccup',\n",
    "                 'PersPerOccupHous', 'PersPerOwnOccHous', 'PersPerRentOccHous', 'PctPersOwnOccup', 'PctPersDenseHous',\n",
    "                 'PctHousLess3BR', 'MedNumBR', 'HousVacant', 'PctHousOccup', 'PctHousOwnOcc', 'PctVacantBoarded',\n",
    "                 'PctVacMore6Mos', 'MedYrHousBuilt', 'PctHousNoPhone', 'PctWOFullPlumb', 'OwnOccLowQuart', 'OwnOccMedVal',\n",
    "                 'OwnOccHiQuart', 'RentLowQ', 'RentMedian', 'RentHighQ', 'MedRent', 'MedRentPctHousInc', 'MedOwnCostPctInc',\n",
    "                 'MedOwnCostPctIncNoMtg', 'NumInShelters', 'NumStreet', 'PctForeignBorn', 'PctBornSameState', 'PctSameHouse85',\n",
    "                 'PctSameCity85', 'PctSameState85', 'LemasSwornFT', 'LemasSwFTPerPop', 'LemasSwFTFieldOps',\n",
    "                 'LemasSwFTFieldPerPop', 'LemasTotalReq', 'LemasTotReqPerPop', 'PolicReqPerOffic',\n",
    "                 'PolicPerPop', 'RacialMatchCommPol', 'PctPolicWhite', 'PctPolicBlack', 'PctPolicHisp',\n",
    "                 'PctPolicAsian', 'PctPolicMinor', 'OfficAssgnDrugUnits', 'NumKindsDrugsSeiz', 'PolicAveOTWorked',\n",
    "                 'LandArea', 'PopDens', 'PctUsePubTrans', 'PolicCars', 'PolicOperBudg', 'LemasPctPolicOnPatr',\n",
    "                 'LemasGangUnitDeploy', 'LemasPctOfficDrugUn', 'PolicBudgPerPop', 'ViolentCrimesPerPop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb673a",
   "metadata": {},
   "source": [
    "## 2.1 Load Data\n",
    "Set the variable `CRIME_FILE` to the **full path** to the forest cover dataset (**crime.csv**) on your system. Load the file into a dataframe (you may initialize the column names using the header list `crime_column_names`), then:\n",
    "1. Determine the number and types of features.\n",
    "2. Perform a **ShuffleSplit** of the data into training/validation/test sets, 60%/20%/20%. \n",
    "3. Split the **non-test data** (*training* + *validation* data) into **5 folds** for cross-validation purposes.\n",
    "4. Perform any necessary preprocessing on dataset. This may include:\n",
    "  * determining if any features should be dropped;\n",
    "  * handling missing data, through imputation and/or complete case analysis. If you **perform imputation on numerical values**, please use **median** imputation.\n",
    "\n",
    "**Please note!** The Scikit-learn function `SimpleImputer` does not work as expected when the feature to impute is numerical but the missing values are not. One way to solve this is to first replace the missing values with NaN values (e.g., `np.nan`) using the Pandas Dataframe method `replace` [(documentation)](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crime_file = f'C:/Users/tanzi/CS Lang IDE/PycharmProjects/Project-2/UTF-8_crime.csv'\n",
    "crime_data = pd.read_csv(crime_file, names=crime_column_names)\n",
    "crime_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Determine the number of features\n",
    "num_features = len(crime_column_names)\n",
    "print('Number of features: ', num_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#change the variable name\n",
    "\n",
    "#Determine nature of object types\n",
    "census_cat_columns=dict()\n",
    "census_cat_columns_idx=dict()\n",
    "census_num_columns_idx=dict()\n",
    "for col in crime_column_names:\n",
    "    if crime_data[col].dtype == 'object':\n",
    "        census_cat_columns[col]=crime_data[col].unique()\n",
    "        census_cat_columns_idx[col]=crime_data.columns.get_loc(col)\n",
    "        print(col,\":\",census_cat_columns[col])\n",
    "    elif np.issubdtype(crime_data.at[0,col], np.number):\n",
    "        census_num_columns_idx[col]=crime_data.columns.get_loc(col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peformance metric functions\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "import numpy as np\n",
    "\n",
    "#A list of keys for the dictionary returned by p1_metrics\n",
    "metric_keys = ['mse','mae','r2']\n",
    "\n",
    "def p2_metrics(y_true,y_pred,negation=False):\n",
    "    if negation:\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "    return {\n",
    "        'mse': sign*mean_squared_error(y_true,y_pred),\n",
    "        'mae': sign*mean_absolute_error(y_true,y_pred),\n",
    "        'r2': sign*r2_score(y_true,y_pred)}\n",
    "\n",
    "#This wrapper can be used to return multiple performance metrics during cross-validation\n",
    "def p2_metrics_scorer(clf,X,y_true):\n",
    "    y_pred=clf.predict(X)\n",
    "    return p2_metrics(y_true,y_pred,negation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c4474",
   "metadata": {},
   "source": [
    "## 2.2 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b802a9fb",
   "metadata": {},
   "source": [
    "1. Construct a linear model using Scikit-learn's `LinearRegression` method with default parameters.\n",
    "2. Report the following performance metrics on the **training and validation sets**:\n",
    "    *Mean Squared Error*, *Mean Absolute Error*, and the *Coefficient of Determination ($r^2$)*.\n",
    "    \n",
    "    You can use the function `p2_metrics` for this purpose. Is this model underfitting the data? Is so, why?\n",
    "3. Report the weights (coefficients) of the linear model and their associated features in ascending order.\n",
    "\n",
    "    Larger weights indicate that their corresponding features have more influence in the model. Moreover, negative weights correspond to variables having negative correlation with the response variable, and vice versa.\n",
    "    \n",
    "    Using this interpretation, describe the most significant features and their correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d09ba6",
   "metadata": {},
   "source": [
    "## 2.3 Linear Regression and PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71808a",
   "metadata": {},
   "source": [
    "1. Perform principal component analysis on the **training data**. You may use Scikit-learn's `PCA` function for this, which **automatically centers** the data prior to PCA. Using PCA, *choose the number of components* for which the total explained variance is $\\ge 99\\%$, and report this.\n",
    "2. After determining the correct number of components, apply the PCA transformation to the **validation** and **test** sets.\n",
    "3. Create another model via `LinearRegression` but using the data transformed by PCA\n",
    "Construct a linear model using Scikit-learn's `LinearRegression` method with default parameters.\n",
    "4. Report the same performance metrics as in 2.2 on the **validation set**. How does the model's performance compare to that of the model in 2.2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09279f2",
   "metadata": {},
   "source": [
    "## 2.4 LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d321fa",
   "metadata": {},
   "source": [
    "Utilizing *cross-validation* you will Construct an $\\ell_1$-regularized linear model using Scikit-learn's `LASSO`:\n",
    "1. Using `GridSearchCV`, determine the best choice of the hyperparameter $\\alpha$ out of values in the list `alphas` below.\n",
    "2. Report the time required to perform cross-validation via `GridSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameter. You may use the function `collate_ht_results` for this purpose.\n",
    "3. Report the weights (coefficients) of the LASSO model and their associated features in ascending order. Note that LASSO attempts to set as many weights to zero in order to create a more parsimonious model while still maintaining regression performance. How many weights are non-zero?\n",
    " \n",
    "### Please Read!\n",
    "There are a few parameters for the `GridSearchCV` and `RandomizedSearchCV` functions that should be set:\n",
    "- `scoring` - This controls the strategy to evaluate the performance of the cross-validated model on the test set, set it to `p2_metrics_scorer`.\n",
    "- `refit` - This will refit an estimator using the best found parameters on the whole dataset, set it to `\"mse\"`\n",
    "- `cv` - This will enable you to reuse your CV splits created in Part 2.1\n",
    "    `n_jobs` - Number of jobs to run in parallel, if you have more than one core on your device (you should), set this to as many as you'd like to use, or to `-1` if you want to use all available cores.\n",
    "- `return_train_score` - Setting this to `False` will reduce computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283aac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10**np.linspace(1,-3,50)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarizes model performance results produced during hyperparameter tuning\n",
    "def collate_ht_results(ht_results,metric_keys=metric_keys,display=True):\n",
    "    ht_stats=dict()\n",
    "    for metric in metric_keys:\n",
    "        ht_stats[metric+\"_mean\"] = ht_results.cv_results_[\"mean_test_\"+metric][ht_results.best_index_]\n",
    "        ht_stats[metric+\"_std\"] = metric_std = ht_results.cv_results_[\"std_test_\"+metric][ht_results.best_index_]\n",
    "        if display:\n",
    "            print(\"test_\"+metric,ht_stats[metric+\"_mean\"],\"(\"+str(ht_stats[metric+\"_std\"])+\")\")\n",
    "    return ht_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1aff9",
   "metadata": {},
   "source": [
    "## 2.5 Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26ebb4",
   "metadata": {},
   "source": [
    "Utilizing *cross-validation* you will construct an MLP regression model using Scikit-learn's `MLPRegressor`:\n",
    "1. Using `GridSearchCV`, determine the best choice of hyperparameters out of the following possible values:\n",
    "- *Number of hidden layers*: [1, 2, 3]\n",
    "- *Number of neurons per layer*: [10, 20, 50]\n",
    "- *Learning rate*: [1e-5, 1e-4, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "\n",
    "2. Report the time required to perform cross-validation via `GridSearchCV`. Report the mean and standard deviation of the performance metrics for the best performing model along with its associated hyperparameter. You may use the function `collate_ht_results` for this purpose.\n",
    "\n",
    " \n",
    "### Please Read!\n",
    "In addition to utilizing the same `GridSearchCV` parameters as in 2.5, the `MLPRegressor` function should have the following parameters set:\n",
    "- `max_iter` -  This controls the maximum number of rounds of backpropagation/gradient descent; set it to 10,000.\n",
    "- `early_stopping` - This will reserve a portion of the training data tha can be used to evaluate convergence progress in order to stop training early; set it to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f4f9a",
   "metadata": {},
   "source": [
    "## 2.6 Final Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b7dcb",
   "metadata": {},
   "source": [
    "1. Using the full training set (**training + validation**), train *two* linear regression models, one with and without PCA preprocessing, then apply them to the test set. For LASSO and MLP, you can utilize the best models found during cross-validation and just apply them to the test set.\n",
    "2. Create a bar chart of the three regression metrics for each model on the same plot.\n",
    "3. How do the models's performances compare? What do the metrics reveal about the dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
